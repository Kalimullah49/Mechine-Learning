# üìä Machine Learning Repository

Welcome to the **Machine Learning Repository** by **Hafiz Kalimullah**! This repository provides a comprehensive guide to building, training, and evaluating machine learning models, making it an ideal resource for both beginners and seasoned practitioners. 

---

## üåê **Repository Highlights**
- **Data Preprocessing**: Handle missing values, outliers, inconsistencies, and more.
- **Supervised Learning**: Tutorials on regression, classification, and advanced techniques.
- **Unsupervised Learning**: Learn clustering, dimensionality reduction, and feature engineering.
- **Model Evaluation**: Understand hyperparameter tuning, cross-validation, and pipelines.
- **Ready-to-Use Models**: Saved models and logs to fast-track your projects.

---

## üîç **What's Inside?**

### **1. CatBoost Logs**
- `catboost_training.json`
- `learn_error.tsv`
- `time_left.tsv`
- `events.out.tfevents`

### **2. Saved Models**
- **Logistic Regression**: `02_model_logistic_regression.pkl`
- **Decision Trees**: `Decision_tree_01.dot`, `Decision_tree_02.dot`, `Decision_tree_03.dot`
- **General Models**: `model_01.pkl`

### **3. Unsupervised Machine Learning**
- **15_k_means_all.ipynb**: K-means clustering.
- **15a_kmeans_paper_2023.pdf**: Research paper on K-means.
- **16-heirarchical-clustering.ipynb**: Hierarchical clustering.
- **17_dbscan_and_optics_clustering.ipynb**: DBSCAN and OPTICS algorithms.
- **18_gmms.ipynb**: Gaussian Mixture Models.
- **19_feature_engineering.ipynb**: Feature engineering techniques.
- **20_PCA.ipynb**: Principal Component Analysis.
- **21_t_SNE.ipynb**: t-SNE for visualization.
- **22_SVD.ipynb**: Singular Value Decomposition.

### **4. Core Machine Learning Pipeline**
1. **Data Preprocessing**:
   - `01_installation.ipynb`: Installation and setup.
   - `02_missing_values.ipynb`: Handling missing data.
   - `03_inconsistencies_in_data.ipynb`: Resolving inconsistencies.
   - `04_outliers.ipynb`: Detecting and dealing with outliers.
   - `05_data_integration.ipynb`: Combining datasets.
   - `06_scaling.ipynb`: Scaling techniques.
   - `07_encoding.ipynb`: Encoding categorical variables.
   - `08_discretization.ipynb`: Discretizing continuous variables.

2. **Supervised Learning Models**:
   - `09_basic_model_coding.ipynb`: Basic machine learning models.
   - `10_linear_regression.ipynb` to `11_logistic_regression.ipynb`: Regression and classification tasks.
   - `12_support_vector_machines.ipynb`: SVMs.
   - `13_knn.ipynb`: K-Nearest Neighbors.
   - `14_decision_tree.ipynb` to `15_random_forest.ipynb`: Tree-based methods.
   - `16_boosting.ipynb`: Boosting techniques.
   - `17_catboost.ipynb`: CatBoost for handling categorical data.

3. **Model Optimization**:
   - `18_hyperparameter_tuning.ipynb`: Tuning hyperparameters.
   - `19_cross_validation.ipynb`: Cross-validation strategies.
   - `20_pipeline.ipynb`: Streamlining workflows with pipelines.
   - `21_best_model_selection.ipynb`: Selecting the best model.

4. **Advanced Topics**:
   - `22_naive_bayes.ipynb`: Probabilistic classification.
   - `23_hyperparamterr_tunning_and_best_model_selection.ipynb`: Combined optimization.
   - `24_impute_missing_values.ipynb`: Imputation strategies.
   - `25_dl_missing_values.ipynb`: Deep learning approaches to missing data.
   - `26_inverse_transform.ipynb`: Reversing transformations.
   - `27_all_models.ipynb`: Comprehensive model comparisons.
   - `28_neural_network.ipynb`: Neural network basics.

---

## üöÄ **Why Choose This Repository?**
- **Complete Workflow**: Covers every step from data preprocessing to model selection.
- **Practical Examples**: Hands-on Jupyter notebooks for real-world tasks.
- **Cutting-Edge Techniques**: Explore advanced topics and methods.

---

## üåü **Getting Started**
1. Clone the repository: `git clone https://github.com/hafizkalimullah/Mechine-Learning`
2. Start exploring the notebooks: `jupyter notebook`

---

üé® **Dive into the world of machine learning and build your expertise today!** üîß
